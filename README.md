# JobDataPipeline

A data pipeline to extract data from Google Jobs Search. 

The end point is a Power BI report that gives insights of the job listings.  


## Project Overview
This project has two primary goals:

**1. Develop skills with various data engineering tools, including:**
   
  - Python
  - SQL
  - Git
  - GitHub
  - Snowflake
  - Airflow
  - Docker
  - Terraform
  - AWS
  - OpenAI GPT3.5 API
  - Selenium Web Driver
  - Power BI
      
**2. Solve a real-world problem:**

Training for a specific job often involves understanding the position and its requirements. A strategic approach is to identify the technologies   and skills required for a particular role. However, manually sifting through job listings can be a time-consuming and exhausting process. To simplify this, the project's solution involves extracting job position data from the internet and compiling it into a structured table with essential information. This data allows for straightforward analysis to identify the most commonly required tools for a given role across all job listings.

The collected information includes:

  - Job Role
  - Company
  - Location (City, Coutry or Remote)
  - Date 
  - Description
  - Tools (Technologies and skills necessary for the job)
  - Years of Experience
  - URL (link to the online job posting on Google's website ) 

The framework for this project is heavily based on [ABZ-Aaron's Reddit-API-Pipeline](https://github.com/ABZ-Aaron/Reddit-API-Pipeline) repository. I'm grateful for his work and it was the best resource I came across to start interacting with data engineering tools.

## Architeture
<img src = "https://github.com/bccestari/JobDataPipeline/blob/main/images/Diagram.png" width = 75% height = 75%>

1. The data pipeline is orchestrated with **Airflow** running in **Docker**.
2. A **webscraper** is deployed within a **Selenium standalone container** to extract data in **.csv** format. 
3. Feature engineering is done using **Python** and a list of words generated by **GPT3.5 API**.
4. The data is loaded into an S3 bucket, which serves as the staging area.
5. **Snowflake** is used for data warehousing.
6. A final report is created using **Power BI**.
7. AWS resources are created/destroyed using **Terraform**.


## Visualization

The final output is a Power BI dashboard:

<img src = "https://github.com/bccestari/JobDataPipeline/blob/main/images/dashboard.png" width = 90% height = 90%>

You can conveniently access the dashboard by following this [link](https://app.powerbi.com/view?r=eyJrIjoiYTdmYmFkZmItMDk5MC00YThhLTgwMTQtZTM4MWNiMmM5MWE4IiwidCI6IjAzZWRlZjRjLWM1MjMtNGYxOC05ZWUyLTkzY2Y5NzIzZWQ0NCJ9).


## Usage

The pipeline was developed at a Linux machine. If the user is on a Mac or Windows, it's necessary make changes.

The free tier of AWS is used to created the s3 bucket. Terraform can create and destroy that resource and prevent spending budget. Obviously, the size of data will be the principal factor in the cost. 

Pre-requisites:

- Create an AWS account 
- Install the requirements
- Install Docker Desktop
- Install and configure Terraform  
- Create a Snowflake account 
- Edit the path in airflow/extraction/dags/extract_job_data_dag.py to the absolute path of airflow/tmp. The part of script:


  ```python
    code_dir = Mount(target='/tmp/raw_data',
                     source='Absolute path to tmp folder', 
                     type='bind')

  ```

1. The first thing is clone the repository and install the requirements:  

  ```bash
  git clone  https://github.com/bccestari/JobDataPipeline.git 
  cd JobDataPipeline 
  pip install -r requirements.txt

  ```


2. Make changes at airflow/extraction/configuration.conf providing the AWS information (bucket name, region, redshift database if any, etc)

3. Make changes at terraform/variables.tf providing information to create and destroy AWS s3 bucket.

4. Init and apply Terraform:

  ```bash
  cd JobDataPipeline/terraform
  terraform init 
  terraform apply  

  ```

4. Go to webscrape directory and built it's image:

  ```bash
  cd JobDataPipeline/webscrape
  docker build -t webscrape-container .  

  ```

5. Start airflow in docker:

  ```bash
  cd JobDataPipeline/airflow
  docker compose up airflow-init
  docker compose up  

  ```

These commands will build airflow's image and deploy it's containers using the docker-compose.yaml file. It will  mount the approriate volumes on host machine to communicate with the containers.The pipeline works with 2 more containers in addition to airflow's container:

- Webscrape Selenium Standalone Container: Deployed to run in "sleep mode". The container waits a command triggered by Airflow's dag to trigger the webscrape bot inside it. The container is killed when the webscrape finishes it's task. 


- Docker-Proxy: This service is used to proxy communication with the Docker daemon to allow external systems to interact with it over a network connection (webscrape container). It's particularly relevant in scenarios where you need to communicate with Docker from a remote client or when using tools that expect Docker to be accessible over a network connection. 


6. Configure connection between Snowfalke Warehousing and s3 Bucket. The s3 bucket used in this project have temporary permissions. So is necessary create policy in AWS IAM. The video follow video is a tutorial of how configure Snowflake connection with S3: [Load Data from Amazon AWS S3 Bucket to Snowflake Data Warehouse](https://www.youtube.com/watch?v=woFc8Om1-kY).

7. [Connect to Snowflake in Power BI Desktop](https://www.youtube.com/watch?v=RDU0JU409Uw)


## Observations

**About the webscraper**

You  can test the webscraper bot  without setting the pipeline. For this:

- Open webscrape/Bot.py
- Comment out the two display lines to temporarily disable the display. However, please remember to uncomment them before proceeding with the creation of the webscraper image below."

  ```python
    #Exclude display line if need run with Chrome Graphical Inteface
    display = Display(visible=0, size=(1024, 768))
    display.start()

  ```

- You can edit webscrape.py initialization function to scrape job's roles you want:

```python
  def __init__(self):
        super().__init__(verbose=True)

        role_names = [
            "Machine Learning Engineer",
            "Data Engineer",
            "Data Analyst",
            "Data Scientist"
        ]
        
```
Here is a demonstration of the webscraper running with graphical interface:

https://github.com/bccestari/JobDataPipeline/assets/136031922/1b05bb24-b8a0-47bd-ae1f-fd2919516082


The webscraper is and adaptation of the code provided by the [AiCore channel](https://www.youtube.com/@AiCore). I would like to express my gratitude to them as well

**Feature Engineering**

The requirements (tools and skills) are extracted with the folllowing process:

- Extract job's description
- Extract from the description the tools need using the tools.txt generated py the two scripts inside /toolsgpt/:
  - generate_tools_list.py: A script that use GPT3.5 API for generate probably tools/technologies required for the job
  - process_tool_list_txt.py: A script that save all this tools into separate lines on a txt file, remove duplicates etc

You can use the tools.txt or modify it to your case depending on the job's rolles edited above. 

To use the two scripts make sure modify .env with your **OPEN AI API KEY**.



## Final Thoughts

The most complex problem I encountered was setting up the Docker containers. Making the webscraper run in this environment can be quite tricky. The only workaround I found was to create a DockerFile to build the Selenium Standalone container and make it wait for a triggering command. The data extracted from WebScrape is saved inside the /tmp folder (a mounted folder between the container and the host machine).

## TO DO
    - Refactor webscrape.py and Bot.py.
    - Expand the amount of data the webscraper can extract per run. 
    - Automate connection between AWS S3 and Snowflake.
    - Integrate tools.txt generation into the pipeline.
    - Make README.md cleaner.

    

    
