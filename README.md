# JobDataPipeline

A data pipeline to extract data from Google Jobs Search. 

The end point is a Power BI report that gives insights of the job listings.  


## Project Overview
This project has two primary goals:

**1. Develop skills with various data engineering tools, including:**
   
   - Python
   - SQL
   - Git
   - GitHub
   - Snowflake
   - Airflow
   - Docker
   - Terraform
   - AWS
   - OpenAI GPT3.5 API
   - Selenium Web Driver
   - Power BI
      
**2. Solve a real-world problem:**

Training for a specific job often involves understanding the position and its requirements. A strategic approach is to identify the technologies and skills required for a particular role. However, manually sifting through job listings can be a time-consuming and exhausting process. To simplify this, the project's solution involves extracting job position data from the internet and compiling it into a structured table with essential information. This data allows for straightforward analysis to identify the most commonly required tools for a given role across all job listings.

The collected information includes:

   - **Job Role**: The job title, e.g: "Data Analyst", "Data Scientist", "Data Engineer" or "Machine Learning Engineer"
   - **Company**: The company that offers the position
   - **Location**: City, Coutry or Remote. 
   - **Date**: The date of extraction 
   - **Description**: The raw description of the job posting 
   - **Tools** 
   - **Years of Experience**
   - **URL**: Link to the job posting 

The framework for this project is heavily based on [ABZ-Aaron's Reddit-API-Pipeline](https://github.com/ABZ-Aaron/Reddit-API-Pipeline) repository. I'm grateful for his work and it was the best resource I came across to start interacting with data engineering tools.

## Architeture
<img src = "https://github.com/bccestari/JobDataPipeline/blob/main/images/Diagram.png" width = 75% height = 75%>

1. The data pipeline is orchestrated with **Airflow** running in **Docker**.
2. A **web scraper** is deployed within a **Selenium standalone container** to extract data in **.csv** format. 
3. Feature engineering is done using **Python** and a list of words generated by **GPT3.5 API**.
4. The data is loaded into an **S3 bucket**, which serves as the staging area.
5. **Snowflake** is used for data warehousing.
6. A final report is created using **Power BI**.
7. AWS resources are created/destroyed using **Terraform**.


## Visualization

The final output is a Power BI dashboard:

<img src = "https://github.com/bccestari/JobDataPipeline/blob/main/images/dashboard.png">

You can conveniently access the dashboard by following this [link](https://app.powerbi.com/view?r=eyJrIjoiYTdmYmFkZmItMDk5MC00YThhLTgwMTQtZTM4MWNiMmM5MWE4IiwidCI6IjAzZWRlZjRjLWM1MjMtNGYxOC05ZWUyLTkzY2Y5NzIzZWQ0NCJ9).


## Usage

The pipeline was developed on a Linux machine. If the user is on a Mac or Windows, it's necessary to make changes.

The free tier of AWS is used to create the S3 bucket. Terraform can create and destroy that resource and prevent exceeding your spending budget. Obviously, the size of the data will be the principal factor in the cost.

Prerequisites:

- Create an [AWS account](https://aws.amazon.com/pt/free/?trk=9eeea834-765c-4895-95ec-d2fb1a1a573d&sc_channel=ps&ef_id=Cj0KCQiAgK2qBhCHARIsAGACuzlB9KKSb78ElXLubEPORIvjzzDOQaewzFvWdwamEGOz137ltI8be2oaAtEWEALw_wcB:G:s&s_kwcid=AL!4422!3!561843094998!p!!g!!amazon%20web%20services!15278604641!130587773020&all-free-tier.sort-by=item.additionalFields.SortRank&all-free-tier.sort-order=asc&awsf.Free%20Tier%20Types=*all&awsf.Free%20Tier%20Categories=*all)
 
- Install [Docker](https://docs.docker.com/get-docker/)
- Install [Docker Compose](https://docs.docker.com/compose/install/) 
- Install and configure [Terraform](https://developer.hashicorp.com/terraform/tutorials/aws-get-started/install-cli)
- Create a [Snowflake account](https://www.snowflake.com/en/) 
- Edit the path in **airflow/extraction/dags/extract_job_data_dag.py** to the absolute path of airflow/tmp. The part of script:


  ```python
    code_dir = Mount(target='/tmp/raw_data',
                     source='Absolute path to tmp folder', 
                     type='bind')

  ```

1. The first thing is clone the repository and install the requirements:  

  ```bash
  git clone  https://github.com/bccestari/JobDataPipeline.git 
  cd JobDataPipeline 
  pip install -r requirements.txt

  ```


2. Make changes at **airflow/extraction/configuration.conf** providing the AWS information (bucket name, region, redshift database if any, etc)
   
         [aws_config]
         bucket_name = XXXXXXXXX
         account_id = XXXXXXXXX
         aws_region = XXXXXXXXX
                  
   
4. Make changes at **terraform/variables.tf** providing information to create and destroy AWS s3 bucket.

5. Initialize and apply **Terraform**:

  ```bash
  cd JobDataPipeline/terraform
  terraform init 
  terraform apply  

  ```

5. Go to webscrape directory and built it's image:

  ```bash
  cd JobDataPipeline/webscrape
  docker build -t webscrape-container .  

  ```

6. Start Airflow in Docker:

  ```bash
  cd JobDataPipeline/airflow
  docker compose up airflow-init
  docker compose up  

  ```

These commands will build Airflow's image, deploy its containers using the docker-compose.yaml file, and mount the appropriate volumes on the host machine to facilitate communication with the containers. The pipeline works with two additional containers:

- **Web scrape using a Selenium Standalone Container**: Deployed to run in a 'sleep mode,' the container waits for a command triggered by Airflow's DAG to initiate the web scraping bot within it. The container is terminated when the web scraping completes its task.
  
- **Docker-Proxy**: This service is used to proxy communication with the Docker daemon, enabling external systems to interact with it over a network connection (e.g., the web scraping container). It's particularly relevant in scenarios where you need to communicate with Docker from a remote client or when using tools that require Docker to be accessible over a network connection.
  
**Observation**:  The above feature needs to be used with caution as it can create a potential security vulnerability. When Docker is accessible over a network, it opens a potential entry point for unauthorized access if not properly configured and secured.


7. Configure the connection between Snowflake Data Warehousing and the S3 Bucket. The S3 bucket used in this project has temporary permissions, so it is necessary to create a policy in AWS IAM. The following video is a tutorial on how to configure the Snowflake connection with S3: [Load Data from Amazon AWS S3 Bucket to Snowflake Data Warehouse](https://www.youtube.com/watch?v=woFc8Om1-kY).

8. [Connect to Snowflake in Power BI Desktop](https://www.youtube.com/watch?v=RDU0JU409Uw)


## Observations

**About the web scraper**

You can test the web scraper bot without configuring the pipeline. To do this:

- Open **webscrape/Bot.py**
- Comment out the two **display lines** to temporarily disable the display. However, please remember to uncomment them before proceeding with the creation of the webs craper image below."

  ```python
    #Exclude display line if need run with Chrome Graphical Inteface
    display = Display(visible=0, size=(1024, 768))
    display.start()

  ```

- You can edit webscrape.py initialization function to scrape job's roles you want:

```python
  def __init__(self):
        super().__init__(verbose=True)

        role_names = [
            "Machine Learning Engineer",
            "Data Engineer",
            "Data Analyst",
            "Data Scientist"
        ]
        
```
The web scraper runs on *port 4444*, but you can change it to any port of your choice.

Here is a demonstration of the web scraper running with graphical interface:

https://github.com/bccestari/JobDataPipeline/assets/136031922/1b05bb24-b8a0-47bd-ae1f-fd2919516082


The web scraper is an adaptation of the code provided by the [AiCore channel](https://www.youtube.com/@AiCore). I would like to express my gratitude to them as well.

**Feature Engineering**

The requirements, including necessary tools and skills, are extracted through the following process:

1. Extract the job's description.
2. From the description, extract the required tools using the **'tools.txt'** generated by the two scripts located in the **'/toolsgpt/'** directory::
  - **generate_tools_list.py**: This script utilizes the GPT3.5 API to generate a list of probable tools/technologies required for the job.
  - **process_tool_list_txt.py**: This script saves all these tools on separate lines in a text file, removing duplicates and performing other necessary tasks.

You can use the **tools.txt** as is or modify it to suit your specific job roles.

To use the two scripts make sure modify **.env** with your **OPEN AI API KEY**.


## Final Thoughts

The most complex problem I encountered was setting up the Docker containers. Running the web scraper in this environment can be quite challenging. The solution I found was to create a Dockerfile to build the Selenium Standalone container and have it wait for a triggering command. The data extracted from the web scrape is saved inside the '/tmp' folder, which is a mounted directory shared between the container and the host machine.

## TO DO
   
   - Refactor webscrape.py and Bot.py.
   - Expand the amount of data the web scraper can extract per run. 
   - Automate connection between AWS S3 and Snowflake.
   - Integrate tools.txt generation into the pipeline.
   - Make README.md cleaner.

    

    
