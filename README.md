# JobDataPipeline

A data pipeline to extract data from Google Jobs Search. 

The end point is a Power BI report that gives insights of the job listings.  


## Project Overview
This project has two primary goals:

**1. Develop skills with various data engineering tools, including:**
   
  - Python
  - SQL
  - Git
  - GitHub
  - Snowflake
  - Airflow
  - Docker
  - Terraform
  - AWS
  - OpenAI GPT3.5 API
  - Selenium Web Driver
  - Power BI
      
**2. Solve a real-world problem:**

Training for a specific job often involves understanding the position and its requirements. A strategic approach is to identify the technologies   and skills required for a particular role. However, manually sifting through job listings can be a time-consuming and exhausting process. To simplify this, the project's solution involves extracting job position data from the internet and compiling it into a structured table with essential information. This data allows for straightforward analysis to identify the most commonly required tools for a given role across all job listings.

The collected information includes:

  - Job Role
  - Company
  - Location (City, Coutry or Remote)
  - Date 
  - Description
  - Tools (Technologies and skills necessary for the job)
  - Years of Experience
  - URL (link to the online job posting on Google's website ) 

The framework for this project is heavily based on [ABZ-Aaron's Reddit-API-Pipeline](https://github.com/ABZ-Aaron/Reddit-API-Pipeline) repository. I'm grateful for his work and it was the best resource I came across to start interacting with data engineering tools." 

## Architeture
<img src = "https://github.com/bccestari/JobDataPipeline/blob/main/images/Diagram.png" width = 75% height = 75%>

1. The data pipeline is orchestrated with **Airflow** running in **Docker**.
2. A **webscraper** is deployed within a **Selenium standalone container** to extract data in **.csv** format. 
3. Feature engineering is done using **Python** and a list of words generated by **GPT3.5 API**.
4. The data is loaded into an S3 bucket, which serves as the staging area.
5. **Snowflake** is used for data warehousing.
6. A final report is created using **Power BI**
7. AWS resources are created/destroyed using **Terraform**.
8. A final report is created using **Power BI**

##Visualization

The final output is a Power BI dashboard:
